constants: null
global_:
  seed: 42
  debug: false
data:
  context_length: 128
  dataset_name: input
  dataset_size: null
  dataset_path: ./data/tinyshakespeare/input.txt
  dataset_dir: ./data/tinyshakespeare
  dataset_url: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
  split: null
  collate_fn: null
  train_loader:
    batch_size: 128
    shuffle: true
    num_workers: 0
    pin_memory: false
    drop_last: false
  valid_loader: null
  test_loader: null
optimizer:
  name: "torch.optim.AdamW"
  lr: 0.0005
  weight_decay: 0.01
criterion:
  name: "torch.nn.CrossEntropyLoss"
  reduction: "mean"
model:
  d_model: 128
  vocab_size: ??? # MISSING so need fill up later
  context_length: ${data.context_length}
  num_decoder_blocks: 5
  dropout: 0.1
  decoder_block:
    masked_self_attention_mha:
      attention:
        _target_: omnivault.transformer.modules.attention.core.ScaledDotProductAttention
      d_model: ${model.d_model}
      H: 8
      dropout: 0.1
    feed_forward:
      d_model: ${model.d_model}
      d_ff: 512
      activation:
        _target_: torch.nn.GELU
        approximate: "tanh"
      dropout: 0.1
      bias: true
    add_norm_1:
      feature_dim: ${model.d_model}
      dropout: 0.1
    add_norm_2:
      feature_dim: ${model.d_model}
      dropout: 0.1
trainer:
  device: "auto"
  max_epochs: 5
  save_every_epoch: true
generator:
  temperature: 1.0
  max_tokens: 1000
  greedy: false
  top_k: 5